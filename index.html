<!DOCTYPE html>
<html>
	<head>
<style>
img {
    display: block;
    margin: 0 auto;
}
</style>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

		<title>Playing the Beer Game Using Reinforcement Learning</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">		
	</head>
	<body>
		<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
        		<li><a href="/about">About us</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
				<h1> Playing the Beer Game Using Reinforcement Learning </h1>
			<h2> The Classical Beer Game </h2>
				<p> The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate a phenomenon known as the bullwhip effect. The game consists of a serial supply chain network with four players&mdash;a retailer, a wholesaler, a distributor, and a manufacturer. <p>
				<!-- alt="The agents of beer game"  -->
				<img src="beer_game_slide.jpg" alt="The agents of beer game" width="583" height="480" align="middle"/> 
				<p> In each period of the game, the retailer experiences a random demand from customers. Then the four players each decide how much inventory of "beer" to order. The retailer orders from the wholesaler, the wholesaler orders from the distributor, the distributor from the manufacturer, and the manufacturer orders from an external supplier that is not a player in the game. If a player does not have sufficient inventory to serve the current period's demand (or orders), the unmet demands are backordered; that is, the customer or downstream node waits until inventory is available.  The manufacturer's supplier, however, never has shortages&mdash;it is assumed to have infinite supply. There are deterministic transportation and information lead times, though the total lead time is stochastic due to stockouts at upstream nodes. Each player may have nonzero shortage and holding costs. <p>
			<p> The players' goal is to minimize the total holding and shortage costs of the whole supply chain. This is therefore a cooperative game, but the players are not allowed to communicate with one another, and moreover they do not know the inventory levels or ordering decisions at the other players&mdash;each player only has access to local information.<p>
			<p> The beer game has been written about quite a bit in the academic literature. Most of this research asks, <b>how do human players play the beer game?</b> and examines the bullwhip effect and other phenomena that arise from this play. An important question that has not been addressed much in the literature is, <b>how <i>should</i> players play the beer game?</b> In other words, what is the optimal inventory policy that a given player should follow? This is a difficult question, especially when the other players do not, themselves, follow an optimal policy. (The beer game can be classified as a multi-agent, cooperative, decentralized, partially observed Markov decision process, a class of problems that is known to be NEXP-complete&mdash;that is, very hard.)<p>
				
			<h2>The Machine Learning Model</h2>
			<p>Our research addresses this question by developing a machine learning (ML) or artificial intelligence (AI) agent to play the beer game. <p>
				<p> We propose a modified Deep Q-Network (DQN) to solve to this problem. DQN has been used to solve single-agent games (such as <a href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/">Atari games</a>) and two-agents zero-sum games (such as <a href="https://deepmind.com/research/alphago/">Go</a>. However, the beer game is a cooperative, non-zero-sum game. Therefore, using DQN directly would result in each player minimizing its own cost, ignoring the system-wide cost. To fix this issue, we propose a feedback scheme that causes the agents to learn to minimize the whole cost of system. To this end, we update the observed reward of each agent <i>i</i> in each period <i>t</i> using</p>
				<img src="http://mathurl.com/y8auqxt8.png">
				<p> in which we penalize the agent using the sum of the costs of the other agents. Therefore, the agents learn to minimize the total cost of the system instead of just their own costs. <p> 


				<!-- alt="results"  -->
			<h2>How Well Does It Play the Game?</h2>
				<p> We present the results from a straightforward version of the game in which the DQN plays the role of the wholesaler and the other three players each follow a base-stock policy. (A base-stock policy means we always order to bring the inventory position&mdash;on-hand inventory minus backorders plus on-order inventory&mdash;equal to a fixed value, called the base-stock level. A base-stock policy is known to be optimal for the beer game, assuming all four players use it.) Each stage has a holding cost of 2 per item, per period. The retailer has a shortage cost of 2 per item per period and the other stages do not have a shortage cost.<p>
			<p>Before the algorithm is trained, the wholesaler plays pretty badly. It stocks way too much inventory, as you can see in the figure below, and causes stockouts at the upstream stages. (Positive inventory is shown as brown boxes and positive numbers. Backorders are shown as red boxes and negative numbers.) <p> 
				<img src="before_training_100.jpg" alt="before training" width="680" height="382" align="middle"/> 
				
				<p> After 1000 training episodes, the DQN agent has learned that there is positive holding cost and so it is better to keep its inventory level smaller. <p> 
				<img src="1000_training_100.jpg" alt="after 1000 training episodes" width="680" height="382" align="middle"/> 

				<p> After 2000 training episodes, it has learned that there is no stock-out cost upstream from the retailer, so it is better to keep its inventory level negative. In general, the whole network is more stable and the inventory levels are all closer to zero than in the previous screenshots.<p> 
				<img src="2000_training_100.jpg" alt="after 2000 training episodes" width="680" height="382" align="middle"/> 

				<p> After 45000 training episodes, the AI agent not only plays in a way to keep its inventory level and cost close to zero, but also to minimize the total cost of the system. In fact, it plays very close to a base-stock policy, which is the optimal policy. The whole network is more stable and the inventory levels are closer to zero than in previous screenshots.<p> 
				<img src="45000_training_100.jpg" alt="after 45000 training episodes" width="680" height="382" align="middle"/> 
			<p>By the way, these screen shots come from a new, free, computerized version of the beer game developed by <a href="http://opexanalytics.com/">Opex Analytics</a>. You can sign up to receive an e-mail when the game is released <a href="http://beergame.opexanalytics.com/">here</a>. 

			<h2>Numerical Results</h2>
				<p>The figures below plot the total cost of the four players for two cases: (1) when all four players use a base-stock policy, which is optimal for this setting and therefore provides a baseline (blue curve), and (2) when one of the players is replaced by the DQN agent (green curve). The x-axis indicates the number of episodes on which the DQN is trained. The upper subfigure indicates the total cost, while the lower subfigure indicates the normalized cost, in which the costs of the BS policy and the DQN are divided by BS policy's cost. The figures show that after a relatively small number of training episodes, the AI has learned to play nearly optimally.<p>
			
			The following figure represents the case in which the DQN plays the <b>retailer:</b>
				<img src="brain3_2_layer_lr000025_multPrd10_dnnUpCnt10000.png" alt="retailer follows DQN" width="680" height="536" align="middle"/> 
				After it has been sufficiently trained, there is roughly an 8% gap compared to the case in which all agents follow a base-stock policy. <p>
			
			<p>Here is the case in which the DQN plays the <b>wholesaler:</b>
				<img src="brain4_3_layer_lr000025_multPrd10_dnnUpCnt10000.png" alt="wholesaler follows DQN" width="680" height="536" align="middle"/> 
				There is an approximately 6% gap compared to the case in which all agents follow a base-stock policy. 
			<p>When the DQN plays the <b>distributor</b>, there is a 7% gap:
				<img src="brain5_3_layer_lr000025_multPrd5_dnnUpCnt10000.png" alt="distributor follows DQN" width="680" height="536" align="middle"/> 
			<p>And finally, when the DQN plays the <b>manufacturer:</b>, there is about a 2% gap compared to the optimal solution:
				<img src="brain6_3_layer_lr000025_multPrd5_dnnUpCnt10000.png" alt="manufacturer follows DQN" width="680" height="536" align="middle"/> 
				

				<p> <em> <b> Conclusion </b> </em> <p>
					These results are really incredible!!!. We can learn to play close to optimal solution, with just&mdash;and only just&mdash;some historical data. Without any knowledge how other agents play, what is their ordering policy, etc. 
					Nowadays that any company has huge datasets, we can easily use those data and optimize their supply chain network. This approach can be easily used in different supply chain with much more complex networks, and more agents. 
					We will post more results latter. BTW, you also can read the corresponding paper <a href="https://arxiv.org/abs/1708.05924">here </a>.!
				<!-- <iframe width="420" height="315" src="https://www.youtube.com/embed/XGSy3_Czz8k"> </iframe> -->				
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
	</body>
</html>
